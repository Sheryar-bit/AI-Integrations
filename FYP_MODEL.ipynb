{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNba1E5qSREfOhOaQLui6gL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8988bdfb75bb4520b3907865fdd388a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_3716b7d7bef94af484c0754036ebb7ee"
          }
        },
        "f4f973fc8e90459db6eb2c52e050095b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b668dded29184aab9d972e5467ef395c",
            "placeholder": "​",
            "style": "IPY_MODEL_403d887f854e496a8352915b17b809d7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "cef59dc764484ee3a58be89b14a571df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c06099ad1b8540429f0c7d29a48d6888",
            "placeholder": "​",
            "style": "IPY_MODEL_276b641fbb29432ca129908ad675e29c",
            "value": ""
          }
        },
        "3f407734ad27416e8d570b9d1b7278df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_581d9dee0bb0412f8d548d670a0d75cd",
            "style": "IPY_MODEL_2f13a71f4d3a4549b5ed681439ad85a9",
            "value": true
          }
        },
        "75cc7ccb92fd4e5da7f8473c93e78527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0fcdd87dbc4c4dfa99ec3faaa1a34758",
            "style": "IPY_MODEL_d566d1f2c9bf43c88747952ba6a6a4a8",
            "tooltip": ""
          }
        },
        "c91f5b5613744766be59a25f8e01e218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2b005d24394ad2b0f462e7e6c6cc25",
            "placeholder": "​",
            "style": "IPY_MODEL_314f44ec9a9e4a96a5475bfc3990390e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "3716b7d7bef94af484c0754036ebb7ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b668dded29184aab9d972e5467ef395c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "403d887f854e496a8352915b17b809d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c06099ad1b8540429f0c7d29a48d6888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276b641fbb29432ca129908ad675e29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "581d9dee0bb0412f8d548d670a0d75cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f13a71f4d3a4549b5ed681439ad85a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fcdd87dbc4c4dfa99ec3faaa1a34758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d566d1f2c9bf43c88747952ba6a6a4a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5e2b005d24394ad2b0f462e7e6c6cc25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314f44ec9a9e4a96a5475bfc3990390e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5958c487ccde4197a84620b72d2a8288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4d0a60de0bc4f65beb7e44dee3c2362",
            "placeholder": "​",
            "style": "IPY_MODEL_7e9def8078d5418c8d9759cfe3e0051e",
            "value": "Connecting..."
          }
        },
        "c4d0a60de0bc4f65beb7e44dee3c2362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9def8078d5418c8d9759cfe3e0051e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheryar-bit/AI-Integrations/blob/main/FYP_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "8988bdfb75bb4520b3907865fdd388a6",
            "f4f973fc8e90459db6eb2c52e050095b",
            "cef59dc764484ee3a58be89b14a571df",
            "3f407734ad27416e8d570b9d1b7278df",
            "75cc7ccb92fd4e5da7f8473c93e78527",
            "c91f5b5613744766be59a25f8e01e218",
            "3716b7d7bef94af484c0754036ebb7ee",
            "b668dded29184aab9d972e5467ef395c",
            "403d887f854e496a8352915b17b809d7",
            "c06099ad1b8540429f0c7d29a48d6888",
            "276b641fbb29432ca129908ad675e29c",
            "581d9dee0bb0412f8d548d670a0d75cd",
            "2f13a71f4d3a4549b5ed681439ad85a9",
            "0fcdd87dbc4c4dfa99ec3faaa1a34758",
            "d566d1f2c9bf43c88747952ba6a6a4a8",
            "5e2b005d24394ad2b0f462e7e6c6cc25",
            "314f44ec9a9e4a96a5475bfc3990390e",
            "5958c487ccde4197a84620b72d2a8288",
            "c4d0a60de0bc4f65beb7e44dee3c2362",
            "7e9def8078d5418c8d9759cfe3e0051e"
          ]
        },
        "id": "8Oll-MecvcGi",
        "outputId": "47e3c35f-e446-4392-c99f-b5708ce4afd3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8988bdfb75bb4520b3907865fdd388a6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -U -q transformers accelerate bitsandbytes peft datasets\n",
        "!pip install -q opencv-python-headless matplotlib\n",
        "\n",
        "!rm -rf diffusers\n",
        "!git clone https://github.com/huggingface/diffusers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MIvYK--4vob9",
        "outputId": "4dde9409-6455-4091-dadc-da80f0fbe065"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/diffusers.git\n",
            "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-xv7h_ur3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-xv7h_ur3\n",
            "  Resolved https://github.com/huggingface/diffusers.git to commit 4b843c8430272644807c845e55ad8aaaa9da0d61\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (8.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (3.20.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers==0.37.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers==0.37.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.37.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.37.0.dev0) (2.5.0)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.37.0.dev0-py3-none-any.whl size=4893097 sha256=fe52712fc43f8090ad769c95cb19c6918fd92938235cd1849210a8fa8ec523a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x1ayx0kn/wheels/23/0f/7d/f97813d265ed0e599a78d83afd4e1925740896ca79b46cccfd\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.36.0\n",
            "    Uninstalling diffusers-0.36.0:\n",
            "      Successfully uninstalled diffusers-0.36.0\n",
            "Successfully installed diffusers-0.37.0.dev0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'diffusers'...\n",
            "remote: Enumerating objects: 119211, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 119211 (delta 93), reused 65 (delta 57), pack-reused 119071 (from 2)\u001b[K\n",
            "Receiving objects: 100% (119211/119211), 93.45 MiB | 23.35 MiB/s, done.\n",
            "Resolving deltas: 100% (89373/89373), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Logos\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/logo_model_output\"\n",
        "\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLV4xBvxxhHu",
        "outputId": "a5374236-bf9a-4cc7-d3bb-9f0fa0c84c4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import json\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
        "\n",
        "metadata = []\n",
        "image_files = [f for f in os.listdir(DATASET_PATH) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "print(f\"Captioning {len(image_files)} images... this may take a few minutes.\")\n",
        "\n",
        "for filename in image_files:\n",
        "    img_path = os.path.join(DATASET_PATH, filename)\n",
        "    raw_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    full_caption = f\"{caption}, professional logo style\"\n",
        "\n",
        "    metadata.append({\"file_name\": filename, \"text\": full_caption})\n",
        "\n",
        "with open(os.path.join(DATASET_PATH, 'metadata.jsonl'), 'w') as f:\n",
        "    for entry in metadata:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(\"Metadata generated successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "xdfV3JTXyMud",
        "outputId": "d56b36c6-050c-4afe-b786-b8313beb30d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3902769990.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlipProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlipForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2682\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config default\n",
        "\n",
        "# Start Training\n",
        "!python /content/diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
        "  --train_data_dir=\"$DATASET_PATH\" \\\n",
        "  --dataloader_num_workers=2 \\\n",
        "  --resolution=512 \\\n",
        "  --center_crop \\\n",
        "  --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --max_train_steps=1500 \\\n",
        "  --learning_rate=1e-04 \\\n",
        "  --max_grad_norm=1 \\\n",
        "  --lr_scheduler=\"cosine\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --output_dir=\"$OUTPUT_DIR\" \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --validation_prompt=\"a minimalist logo for a tech company, clean lines\" \\\n",
        "  --seed=42 \\\n",
        "  --mixed_precision=\"fp16\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P8d44BLL0xBU",
        "outputId": "e1f22770-1387-4d6f-e497-7b970b5c1e97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
            "2026-01-20 14:37:57.909175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768919877.975097    6673 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768919877.985516    6673 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768919878.065767    6673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768919878.065811    6673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768919878.065817    6673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768919878.065821    6673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-20 14:37:58.084860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 1.63MB/s]\n",
            "{'clip_sample_range', 'thresholding', 'dynamic_thresholding_ratio', 'sample_max_value', 'variance_type', 'rescale_betas_zero_snr', 'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 6.07MB/s]\n",
            "vocab.json: 1.06MB [00:00, 5.63MB/s]\n",
            "merges.txt: 525kB [00:00, 7.89MB/s]\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.76MB/s]\n",
            "config.json: 100% 617/617 [00:00<00:00, 5.02MB/s]\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:04<00:00, 107MB/s]\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.17MB/s]\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 185MB/s]\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "config.json: 100% 743/743 [00:00<00:00, 5.66MB/s]\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:30<00:00, 111MB/s]\n",
            "{'conv_in_kernel', 'time_embedding_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'time_embedding_dim', 'class_embeddings_concat', 'resnet_out_scale_factor', 'projection_class_embeddings_input_dim', 'time_embedding_act_fn', 'addition_embed_type', 'num_class_embeds', 'timestep_post_act', 'dropout', 'conv_out_kernel', 'only_cross_attention', 'attention_type', 'num_attention_heads', 'transformer_layers_per_block', 'mid_block_only_cross_attention', 'encoder_hid_dim_type', 'dual_cross_attention', 'upcast_attention', 'class_embed_type', 'time_cond_proj_dim', 'use_linear_projection', 'resnet_skip_time_act', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'mid_block_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Resolving data files: 100% 1472/1472 [00:00<00:00, 12152.16it/s]\n",
            "Downloading data: 100% 1437/1437 [00:00<00:00, 13991.01files/s]\n",
            "Generating train split: 1435 examples [00:00, 19334.55 examples/s]\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 1435\n",
            "INFO:__main__:  Num Epochs = 5\n",
            "INFO:__main__:  Instantaneous batch size per device = 1\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = 1500\n",
            "Steps:  24% 359/1500 [08:54<25:53,  1.36s/it, lr=8.65e-5, step_loss=0.00428]\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 3.13MB/s]\n",
            "\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 2.38MB/s]\n",
            "\n",
            "Fetching 13 files:   8% 1/13 [00:00<00:11,  1.00it/s]\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "config.json: 4.72kB [00:00, 16.0MB/s]\n",
            "\n",
            "Fetching 13 files:  23% 3/13 [00:01<00:03,  3.10it/s]\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:   0% 67.8k/1.22G [00:01<9:53:00, 34.2kB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:   1% 17.4M/1.22G [00:02<02:43, 7.32MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:   7% 84.5M/1.22G [00:03<00:31, 36.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  12% 143M/1.22G [00:04<00:20, 53.2MB/s] \u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  17% 210M/1.22G [00:04<00:12, 81.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  28% 344M/1.22G [00:04<00:05, 153MB/s] \u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  34% 412M/1.22G [00:06<00:11, 70.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  39% 479M/1.22G [00:07<00:07, 93.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  45% 546M/1.22G [00:07<00:05, 120MB/s] \u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  50% 613M/1.22G [00:07<00:04, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  56% 680M/1.22G [00:07<00:02, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  61% 747M/1.22G [00:07<00:02, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  67% 814M/1.22G [00:08<00:01, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  72% 881M/1.22G [00:08<00:01, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  78% 948M/1.22G [00:08<00:01, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  83% 1.01G/1.22G [00:08<00:00, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  89% 1.08G/1.22G [00:11<00:01, 80.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  94% 1.15G/1.22G [00:11<00:00, 106MB/s] \u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors: 100% 1.22G/1.22G [00:11<00:00, 103MB/s]\n",
            "\n",
            "Fetching 13 files: 100% 13/13 [00:12<00:00,  1.01it/s]\n",
            "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:02<00:05,  1.02s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  57% 4/7 [00:02<00:01,  2.19it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  86% 6/7 [00:04<00:00,  1.36it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:10<00:00,  1.46s/it]\n",
            "INFO:__main__:Running validation... \n",
            " Generating 4 images with prompt: a minimalist logo for a tech company, clean lines.\n",
            "Steps:  33% 500/1500 [13:06<24:00,  1.44s/it, lr=7.51e-5, step_loss=0.0354]INFO:accelerate.accelerator:Saving current state to /content/drive/MyDrive/logo_model_output/checkpoint-500\n",
            "Model weights saved in /content/drive/MyDrive/logo_model_output/checkpoint-500/pytorch_lora_weights.safetensors\n",
            "INFO:accelerate.checkpointing:Optimizer state saved in /content/drive/MyDrive/logo_model_output/checkpoint-500/optimizer.bin\n",
            "INFO:accelerate.checkpointing:Scheduler state saved in /content/drive/MyDrive/logo_model_output/checkpoint-500/scheduler.bin\n",
            "INFO:accelerate.checkpointing:Sampler state for dataloader 0 saved in /content/drive/MyDrive/logo_model_output/checkpoint-500/sampler.bin\n",
            "INFO:accelerate.checkpointing:Gradient scaler state saved in /content/drive/MyDrive/logo_model_output/checkpoint-500/scaler.pt\n",
            "INFO:accelerate.checkpointing:Random states saved in /content/drive/MyDrive/logo_model_output/checkpoint-500/random_states_0.pkl\n",
            "INFO:__main__:Saved state to /content/drive/MyDrive/logo_model_output/checkpoint-500\n",
            "Steps:  48% 718/1500 [18:20<17:52,  1.37s/it, lr=5.33e-5, step_loss=0.0127]{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:01<00:04,  1.19it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:03<00:01,  1.28it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:04<00:00,  1.42it/s]\n",
            "INFO:__main__:Running validation... \n",
            " Generating 4 images with prompt: a minimalist logo for a tech company, clean lines.\n",
            "Steps:  67% 1000/1500 [25:35<12:00,  1.44s/it, lr=2.51e-5, step_loss=0.00221]INFO:accelerate.accelerator:Saving current state to /content/drive/MyDrive/logo_model_output/checkpoint-1000\n",
            "Model weights saved in /content/drive/MyDrive/logo_model_output/checkpoint-1000/pytorch_lora_weights.safetensors\n",
            "INFO:accelerate.checkpointing:Optimizer state saved in /content/drive/MyDrive/logo_model_output/checkpoint-1000/optimizer.bin\n",
            "INFO:accelerate.checkpointing:Scheduler state saved in /content/drive/MyDrive/logo_model_output/checkpoint-1000/scheduler.bin\n",
            "INFO:accelerate.checkpointing:Sampler state for dataloader 0 saved in /content/drive/MyDrive/logo_model_output/checkpoint-1000/sampler.bin\n",
            "INFO:accelerate.checkpointing:Gradient scaler state saved in /content/drive/MyDrive/logo_model_output/checkpoint-1000/scaler.pt\n",
            "INFO:accelerate.checkpointing:Random states saved in /content/drive/MyDrive/logo_model_output/checkpoint-1000/random_states_0.pkl\n",
            "INFO:__main__:Saved state to /content/drive/MyDrive/logo_model_output/checkpoint-1000\n",
            "Steps:  72% 1077/1500 [27:26<09:34,  1.36s/it, lr=1.84e-5, step_loss=0.0146] {'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:00<00:00, 10.34it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:00<00:00,  9.62it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  6.00it/s]\n",
            "INFO:__main__:Running validation... \n",
            " Generating 4 images with prompt: a minimalist logo for a tech company, clean lines.\n",
            "Steps:  96% 1436/1500 [36:28<01:26,  1.35s/it, lr=4.49e-7, step_loss=0.0168]{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:00<00:00,  9.10it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:00,  9.35it/s]\u001b[A{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:00<00:00,  7.43it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.78it/s]\n",
            "INFO:__main__:Running validation... \n",
            " Generating 4 images with prompt: a minimalist logo for a tech company, clean lines.\n",
            "Steps: 100% 1500/1500 [38:26<00:00,  1.44s/it, lr=1.1e-10, step_loss=0.0489]INFO:accelerate.accelerator:Saving current state to /content/drive/MyDrive/logo_model_output/checkpoint-1500\n",
            "Model weights saved in /content/drive/MyDrive/logo_model_output/checkpoint-1500/pytorch_lora_weights.safetensors\n",
            "INFO:accelerate.checkpointing:Optimizer state saved in /content/drive/MyDrive/logo_model_output/checkpoint-1500/optimizer.bin\n",
            "INFO:accelerate.checkpointing:Scheduler state saved in /content/drive/MyDrive/logo_model_output/checkpoint-1500/scheduler.bin\n",
            "INFO:accelerate.checkpointing:Sampler state for dataloader 0 saved in /content/drive/MyDrive/logo_model_output/checkpoint-1500/sampler.bin\n",
            "INFO:accelerate.checkpointing:Gradient scaler state saved in /content/drive/MyDrive/logo_model_output/checkpoint-1500/scaler.pt\n",
            "INFO:accelerate.checkpointing:Random states saved in /content/drive/MyDrive/logo_model_output/checkpoint-1500/random_states_0.pkl\n",
            "INFO:__main__:Saved state to /content/drive/MyDrive/logo_model_output/checkpoint-1500\n",
            "Steps: 100% 1500/1500 [38:27<00:00,  1.44s/it, lr=0, step_loss=0.155]       {'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:00<00:00,  8.62it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:00,  8.47it/s]\u001b[A{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:00<00:00,  5.50it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.74it/s]\n",
            "INFO:__main__:Running validation... \n",
            " Generating 4 images with prompt: a minimalist logo for a tech company, clean lines.\n",
            "Model weights saved in /content/drive/MyDrive/logo_model_output/pytorch_lora_weights.safetensors\n",
            "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
            "{'conv_in_kernel', 'time_embedding_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'time_embedding_dim', 'class_embeddings_concat', 'resnet_out_scale_factor', 'projection_class_embeddings_input_dim', 'time_embedding_act_fn', 'addition_embed_type', 'num_class_embeds', 'timestep_post_act', 'dropout', 'conv_out_kernel', 'only_cross_attention', 'attention_type', 'num_attention_heads', 'transformer_layers_per_block', 'mid_block_only_cross_attention', 'encoder_hid_dim_type', 'dual_cross_attention', 'upcast_attention', 'class_embed_type', 'time_cond_proj_dim', 'use_linear_projection', 'resnet_skip_time_act', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'mid_block_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/unet.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:15<01:33, 15.64s/it]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'use_post_quant_conv', 'shift_factor', 'use_quant_conv', 'latents_std', 'scaling_factor', 'latents_mean', 'force_upcast', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  29% 2/7 [00:15<00:32,  6.54s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:16<00:03,  1.94s/it]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:16<00:00,  2.39s/it]\n",
            "Loading unet.\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "INFO:__main__:Running validation... \n",
            " Generating 4 images with prompt: a minimalist logo for a tech company, clean lines.\n",
            "Steps: 100% 1500/1500 [39:35<00:00,  1.58s/it, lr=0, step_loss=0.155]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Untrained base model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "# trained LoRA weights\n",
        "pipe.load_lora_weights(OUTPUT_DIR)\n",
        "\n",
        "prompt = \"a modern logo for a Elite Spots Brand. \"\n",
        "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
        "\n",
        "image.save(\"generated_logo.png\")\n",
        "image"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ONwunN6M_6Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "prompt = \"a modern logo for a Elite Spots Brand\"\n",
        "seed = 42 # Using the exact same seed is the \"secret\" to a fair test\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "# UNTRAINED (Base) Model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "print(\"Generating image with UNTRAINED model...\")\n",
        "image_untrained = pipe(prompt, generator=generator, num_inference_steps=30).images[0]\n",
        "\n",
        "#(Trained)\n",
        "print(\"Loading your LoRA weights...\")\n",
        "pipe.load_lora_weights(OUTPUT_DIR)\n",
        "# Reset the generator so the noise starts exactly the same\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "print(\"Generating image with TRAINED model...\")\n",
        "image_trained = pipe(prompt, generator=generator, num_inference_steps=30).images[0]\n",
        "\n",
        "\n",
        "comparison = Image.new('RGB', (1024, 512))\n",
        "comparison.paste(image_untrained, (0, 0))\n",
        "comparison.paste(image_trained, (512, 0))\n",
        "\n",
        "print(\"Left: Untrained (Base SD 1.5) | Right: Your Trained Logo Model\")\n",
        "comparison.save(\"comparison_result.png\")\n",
        "display(comparison)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IEGttgmeBR8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.set_adapters([\"default\"], adapter_weights=[0.6])\n",
        "\n",
        "prompt = \"professional logo style, minimalist mountain, vector art\"\n",
        "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.0).images[0]\n",
        "display(image)"
      ],
      "metadata": {
        "id": "SrPUQUMOL9dk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}